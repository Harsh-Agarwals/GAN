{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f38f960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import  transforms\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c3ba48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202599"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/img_align_celeba\"\n",
    "\n",
    "len(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21fcbad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.jpg'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i[i.index('.'):] for i in os.listdir(data_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f7796c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=64, interpolation=bilinear, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(64, 64))\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "img_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed6d2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebImages(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_path = [os.path.join(root_dir, img) for img in os.listdir(self.root_dir) if img.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.image_path[index]\n",
    "        img = Image.open(img)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84be8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "celebs = CelebImages(root_dir=data_dir, transforms=img_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdc1a2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202599, torch.Size([3, 64, 64]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(celebs), celebs[9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3426eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9843137..1.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(celebs[0].permute(1, 2, 0))\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ada687fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x12e50c1a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(dataset=celebs, batch_size=128, shuffle=True)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06ac13e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 64, 64]), 1583)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = next(iter(loader))\n",
    "X0.shape, len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281de2e",
   "metadata": {},
   "source": [
    "**Now our dataset is ready, let's build a good Convolutional GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f68555fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa235724",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "57b26af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorNN(\n",
       "  (noise_layer): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (generator_conv): Sequential(\n",
       "    (0): ConvTranspose2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GeneratorNN(nn.Module):\n",
    "    def __init__(self, z_dim, num_channeles):\n",
    "        super().__init__()\n",
    "        self.noise_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=z_dim, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=8192),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.generator_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=num_channeles, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.noise_layer(z)\n",
    "        x = x.view(x.shape[0], 128, 8, 8)\n",
    "        return self.generator_conv(x)\n",
    "    \n",
    "Generator = GeneratorNN(z_dim=100, num_channeles=3)\n",
    "Generator.to(device=device)\n",
    "\n",
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "480d4973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "GeneratorNN                              [128, 100]                [128, 3, 64, 64]          --                        True\n",
       "├─Sequential: 1-1                        [128, 100]                [128, 8192]               --                        True\n",
       "│    └─Linear: 2-1                       [128, 100]                [128, 512]                51,712                    True\n",
       "│    └─ReLU: 2-2                         [128, 512]                [128, 512]                --                        --\n",
       "│    └─Linear: 2-3                       [128, 512]                [128, 2048]               1,050,624                 True\n",
       "│    └─ReLU: 2-4                         [128, 2048]               [128, 2048]               --                        --\n",
       "│    └─Linear: 2-5                       [128, 2048]               [128, 8192]               16,785,408                True\n",
       "│    └─ReLU: 2-6                         [128, 8192]               [128, 8192]               --                        --\n",
       "├─Sequential: 1-2                        [128, 128, 8, 8]          [128, 3, 64, 64]          --                        True\n",
       "│    └─ConvTranspose2d: 2-7              [128, 128, 8, 8]          [128, 256, 16, 16]        295,168                   True\n",
       "│    └─ReLU: 2-8                         [128, 256, 16, 16]        [128, 256, 16, 16]        --                        --\n",
       "│    └─ConvTranspose2d: 2-9              [128, 256, 16, 16]        [128, 128, 32, 32]        295,040                   True\n",
       "│    └─ReLU: 2-10                        [128, 128, 32, 32]        [128, 128, 32, 32]        --                        --\n",
       "│    └─ConvTranspose2d: 2-11             [128, 128, 32, 32]        [128, 64, 64, 64]         73,792                    True\n",
       "│    └─ReLU: 2-12                        [128, 64, 64, 64]         [128, 64, 64, 64]         --                        --\n",
       "│    └─Conv2d: 2-13                      [128, 64, 64, 64]         [128, 3, 64, 64]          1,731                     True\n",
       "│    └─Tanh: 2-14                        [128, 3, 64, 64]          [128, 3, 64, 64]          --                        --\n",
       "============================================================================================================================================\n",
       "Total params: 18,553,475\n",
       "Trainable params: 18,553,475\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 90.23\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 493.36\n",
       "Params size (MB): 74.21\n",
       "Estimated Total Size (MB): 567.62\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=Generator, input_size=(128, 100), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "099957db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 64, 64])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(size=(128, 100))\n",
    "Generator(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55869cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscrimiatorNN(\n",
       "  (discriminator_nn): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): Linear(in_features=2048, out_features=64, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiscrimiatorNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminator_nn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=8192, out_features=2048),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Linear(in_features=2048, out_features=64),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.discriminator_nn(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "Discrimiator = DiscrimiatorNN()\n",
    "Discrimiator.to(device=device)\n",
    "\n",
    "Discrimiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b2371926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DiscrimiatorNN                           [128, 1]                  --\n",
       "├─Sequential: 1-1                        [128, 128, 8, 8]          --\n",
       "│    └─Conv2d: 2-1                       [128, 64, 64, 64]         1,792\n",
       "│    └─LeakyReLU: 2-2                    [128, 64, 64, 64]         --\n",
       "│    └─MaxPool2d: 2-3                    [128, 64, 32, 32]         --\n",
       "│    └─Conv2d: 2-4                       [128, 128, 32, 32]        73,856\n",
       "│    └─LeakyReLU: 2-5                    [128, 128, 32, 32]        --\n",
       "│    └─MaxPool2d: 2-6                    [128, 128, 16, 16]        --\n",
       "│    └─Conv2d: 2-7                       [128, 128, 16, 16]        147,584\n",
       "│    └─LeakyReLU: 2-8                    [128, 128, 16, 16]        --\n",
       "│    └─MaxPool2d: 2-9                    [128, 128, 8, 8]          --\n",
       "├─Sequential: 1-2                        [128, 1]                  --\n",
       "│    └─Flatten: 2-10                     [128, 8192]               --\n",
       "│    └─Linear: 2-11                      [128, 2048]               16,779,264\n",
       "│    └─LeakyReLU: 2-12                   [128, 2048]               --\n",
       "│    └─Linear: 2-13                      [128, 64]                 131,136\n",
       "│    └─LeakyReLU: 2-14                   [128, 64]                 --\n",
       "│    └─Linear: 2-15                      [128, 1]                  65\n",
       "│    └─Sigmoid: 2-16                     [128, 1]                  --\n",
       "==========================================================================================\n",
       "Total params: 17,133,697\n",
       "Trainable params: 17,133,697\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 17.62\n",
       "==========================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 438.37\n",
       "Params size (MB): 68.53\n",
       "Estimated Total Size (MB): 513.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=Discrimiator, input_size=(128, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5f407f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discrimiator(torch.randn(size=(128, 3, 64, 64))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ec037e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GeneratorNN(\n",
       "   (noise_layer): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): Linear(in_features=512, out_features=2048, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "     (5): ReLU()\n",
       "   )\n",
       "   (generator_conv): Sequential(\n",
       "     (0): ConvTranspose2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "     (1): ReLU()\n",
       "     (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "     (3): ReLU()\n",
       "     (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "     (5): ReLU()\n",
       "     (6): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (7): Tanh()\n",
       "   )\n",
       " ),\n",
       " DiscrimiatorNN(\n",
       "   (discriminator_nn): Sequential(\n",
       "     (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   )\n",
       "   (classifier): Sequential(\n",
       "     (0): Flatten(start_dim=1, end_dim=-1)\n",
       "     (1): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "     (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (3): Linear(in_features=2048, out_features=64, bias=True)\n",
       "     (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (5): Linear(in_features=64, out_features=1, bias=True)\n",
       "     (6): Sigmoid()\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator, Discrimiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b24ab16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BCEWithLogitsLoss(),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(params=Generator.parameters(), lr=1e-3)\n",
    "discriminator_optimizer = torch.optim.Adam(params=Discrimiator.parameters(), lr=1e-3)\n",
    "\n",
    "gan_loss, generator_optimizer, discriminator_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e22a4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9de3b9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, D loss: 0.7217370271682739, G loss: 0.44871920347213745\n",
      "Batch: 200, D loss: 0.8132616281509399, G loss: 0.31326165795326233\n",
      "Batch: 400, D loss: 0.8132616281509399, G loss: 0.31326165795326233\n",
      "Batch: 600, D loss: 0.8132616281509399, G loss: 0.31326165795326233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[161]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m Discrimiator = Discrimiator.to(device=device)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/silvercule/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/silvercule/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/silvercule/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mCelebImages.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     13\u001b[39m img = Image.open(img)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/silvercule/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/silvercule/lib/python3.13/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/silvercule/lib/python3.13/site-packages/torchvision/transforms/functional.py:174\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    172\u001b[39m img = img.view(pic.size[\u001b[32m1\u001b[39m], pic.size[\u001b[32m0\u001b[39m], F_pil.get_image_num_channels(pic))\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img.to(dtype=default_float_dtype).div(\u001b[32m255\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "Generator = Generator.to(device=device)\n",
    "Discrimiator = Discrimiator.to(device=device)\n",
    "for epoch in range(epochs):\n",
    "    for batch, X in enumerate(loader):\n",
    "        batch_size = X.shape[0]\n",
    "        X = X.to(device=device)\n",
    "        \n",
    "        reals = torch.ones(size=(batch_size, 1)).to(device=device)\n",
    "        fakes = torch.zeros(size=(batch_size, 1)).to(device=device)\n",
    "\n",
    "        # Train discriminators\n",
    "        Discrimiator.train()\n",
    "        real_loss = gan_loss(Discrimiator(X), reals)\n",
    "        fake_loss = gan_loss(Discrimiator(Generator(torch.randn(size=(batch_size, 100)).to(device=device).detach())), fakes)\n",
    "        disc_loss = (real_loss + fake_loss) / 2\n",
    "        disc_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        \n",
    "        # Train generators\n",
    "        Generator.train()\n",
    "        gen_loss = gan_loss(Discrimiator(Generator((torch.randn(size=(batch_size, 100)).to(device=device).detach()))), reals)\n",
    "        gen_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        if batch%200==0:\n",
    "            print(f\"Batch: {batch}, D loss: {disc_loss}, G loss: {gen_loss}\")\n",
    "\n",
    "    z = torch.randn(size=(36, 100)).to(device=device)\n",
    "    epoch_res = Generator(z).cpu().detach()\n",
    "    grid = torchvision.utils.make_grid(epoch_res, nrow=4, normalize=True)\n",
    "    plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "    plt.title(f\"Result after epoch {epoch}\")\n",
    "    plt.axis(False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf08eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silvercule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
